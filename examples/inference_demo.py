#!/usr/bin/env python3
"""
Brain-Inspired AI - Inference Demo

This script demonstrates how to:
1. Load a trained model
2. Perform single-modality inference
3. Perform multi-modal inference
4. Use different output types (classify, generate, control)
5. Access internal brain states
6. Output detailed model introspection for LLM analysis

Run: python examples/inference_demo.py
"""

import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, Optional, List
from collections import OrderedDict
import json
import gc


def cleanup_memory():
    """Force garbage collection and clear CUDA cache."""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


# =============================================================================
# MODEL INTROSPECTION UTILITIES
# =============================================================================

def count_parameters(model: nn.Module, trainable_only: bool = True) -> int:
    """Count model parameters."""
    if trainable_only:
        return sum(p.numel() for p in model.parameters() if p.requires_grad)
    return sum(p.numel() for p in model.parameters())


def format_param_count(count: int) -> str:
    """Format parameter count in human-readable form."""
    if count >= 1e9:
        return f"{count / 1e9:.2f}B"
    elif count >= 1e6:
        return f"{count / 1e6:.2f}M"
    elif count >= 1e3:
        return f"{count / 1e3:.2f}K"
    return str(count)


def get_layer_info(model: nn.Module) -> Dict[str, Any]:
    """Extract detailed layer information from model."""
    layer_info = OrderedDict()
    
    for name, module in model.named_modules():
        if name == '':
            continue
        
        module_type = type(module).__name__
        params = sum(p.numel() for p in module.parameters(recurse=False))
        
        if params > 0 or module_type in ['Sequential', 'ModuleDict', 'ModuleList']:
            layer_info[name] = {
                'type': module_type,
                'params': params,
                'params_formatted': format_param_count(params),
            }
            
            # Add dimension info for common layers
            if hasattr(module, 'in_features') and hasattr(module, 'out_features'):
                layer_info[name]['shape'] = f"{module.in_features} â†’ {module.out_features}"
            elif hasattr(module, 'in_channels') and hasattr(module, 'out_channels'):
                layer_info[name]['shape'] = f"{module.in_channels}ch â†’ {module.out_channels}ch"
            elif hasattr(module, 'num_embeddings') and hasattr(module, 'embedding_dim'):
                layer_info[name]['shape'] = f"vocab={module.num_embeddings}, dim={module.embedding_dim}"
            elif hasattr(module, 'normalized_shape'):
                layer_info[name]['shape'] = f"norm={module.normalized_shape}"
    
    return layer_info


def get_component_breakdown(model) -> Dict[str, Dict[str, Any]]:
    """Get parameter breakdown by major component."""
    components = {}
    
    # Check for common brain_ai components
    component_names = [
        ('encoders', 'Modality Encoders'),
        ('workspace', 'Global Workspace'),
        ('htm', 'HTM Temporal Layer'),
        ('decision_heads', 'Decision Heads'),
        ('active_inference', 'Active Inference Agent'),
        ('reasoner', 'Symbolic Reasoner'),
        ('neuromodulator', 'Neuromodulatory Gate'),
        ('snn', 'Spiking Neural Network'),
    ]
    
    for attr_name, display_name in component_names:
        if hasattr(model, attr_name):
            component = getattr(model, attr_name)
            if component is not None:
                if isinstance(component, nn.ModuleDict):
                    total_params = sum(count_parameters(m) for m in component.values())
                    sub_components = {k: format_param_count(count_parameters(v)) 
                                      for k, v in component.items()}
                    components[display_name] = {
                        'params': total_params,
                        'params_formatted': format_param_count(total_params),
                        'sub_components': sub_components,
                    }
                elif isinstance(component, nn.Module):
                    params = count_parameters(component)
                    components[display_name] = {
                        'params': params,
                        'params_formatted': format_param_count(params),
                    }
    
    return components


def get_config_details(model) -> Dict[str, Any]:
    """Extract configuration details from model."""
    config_info = {}
    
    if hasattr(model, 'config'):
        cfg = model.config
        
        # Core settings
        config_info['core'] = {
            'modalities': getattr(model, 'modalities', []),
            'output_type': getattr(model, 'output_type', 'unknown'),
            'use_htm': getattr(cfg, 'use_htm', False),
            'use_workspace': getattr(cfg, 'use_workspace', True),
            'use_symbolic': getattr(cfg, 'use_symbolic', False),
            'use_meta': getattr(cfg, 'use_meta', False),
            'use_engram': getattr(cfg, 'use_engram', False),
        }
        
        # SNN config
        if hasattr(cfg, 'snn'):
            config_info['snn'] = {
                'beta': cfg.snn.beta,
                'num_timesteps': cfg.snn.num_timesteps,
                'surrogate': cfg.snn.surrogate,
                'hidden_sizes': cfg.snn.hidden_sizes,
            }
        
        # Encoder config
        if hasattr(cfg, 'encoder'):
            config_info['encoder'] = {
                'output_dim': cfg.encoder.output_dim,
                'vision_channels': getattr(cfg.encoder, 'vision_channels', None),
                'text_vocab_size': getattr(cfg.encoder, 'text_vocab_size', None),
                'text_embed_dim': getattr(cfg.encoder, 'text_embed_dim', None),
                'audio_n_mels': getattr(cfg.encoder, 'audio_n_mels', None),
            }
        
        # HTM config
        if hasattr(cfg, 'htm'):
            config_info['htm'] = {
                'column_count': cfg.htm.column_count,
                'cells_per_column': cfg.htm.cells_per_column,
                'sparsity': cfg.htm.sparsity,
            }
        
        # Workspace config
        if hasattr(cfg, 'workspace'):
            config_info['workspace'] = {
                'workspace_dim': cfg.workspace.workspace_dim,
                'num_heads': cfg.workspace.num_heads,
                'capacity_limit': cfg.workspace.capacity_limit,
                'memory_mode': cfg.workspace.memory_mode,
            }
        
        # Decision config
        if hasattr(cfg, 'decision'):
            config_info['decision'] = {
                'num_classes': cfg.decision.num_classes,
                'control_dim': cfg.decision.control_dim,
            }
    
    return config_info


def get_dataset_requirements(model) -> Dict[str, Any]:
    """Get recommended dataset requirements based on model configuration."""
    total_params = count_parameters(model)
    modalities = getattr(model, 'modalities', [])
    
    # Chinchilla optimal: 20 tokens per parameter
    chinchilla_tokens = total_params * 20
    # Modern practice: 50-100+ tokens per parameter
    modern_tokens = total_params * 100
    
    requirements = {
        'model_params': total_params,
        'model_params_formatted': format_param_count(total_params),
        'chinchilla_optimal_tokens': format_param_count(chinchilla_tokens),
        'modern_recommended_tokens': format_param_count(modern_tokens),
        'per_modality': {},
    }
    
    # Per-modality recommendations
    if 'vision' in modalities:
        requirements['per_modality']['vision'] = {
            'recommended_datasets': [
                'ImageNet-1K (1.2M images) - classification baseline',
                'ImageNet-21K (14M images) - production scale',
                'LAION-400M - web-scale pretraining',
                'DataComp - curated high-quality images',
                'COCO (330K images) - detection/segmentation',
            ],
            'min_samples': '100K images for basic training',
            'recommended_samples': '1M+ images for production',
            'augmentation': 'RandAugment, MixUp, CutMix, ColorJitter',
        }
    
    if 'text' in modalities:
        requirements['per_modality']['text'] = {
            'recommended_datasets': [
                'FineWeb (15T tokens) - high-quality web text',
                'RedPajama (1.2T tokens) - open reproduction of LLaMA data',
                'The Pile (825GB) - diverse curated corpus',
                'C4 (750GB) - cleaned Common Crawl',
                'Wikipedia + BookCorpus - foundational NLP',
            ],
            'min_tokens': '1B tokens for basic fluency',
            'recommended_tokens': f'{format_param_count(modern_tokens)} tokens (100x params)',
            'preprocessing': 'BPE tokenization, deduplication, quality filtering',
        }
    
    if 'audio' in modalities:
        requirements['per_modality']['audio'] = {
            'recommended_datasets': [
                'LibriSpeech (960h) - clean speech recognition',
                'Common Voice (10K+ hours) - multilingual speech',
                'VoxCeleb (2K+ hours) - speaker recognition',
                'AudioSet (5.8K hours) - audio event detection',
                'MusicNet - music understanding',
            ],
            'min_hours': '100 hours for basic ASR',
            'recommended_hours': '10K+ hours for production',
            'preprocessing': 'Mel spectrograms, SpecAugment, noise injection',
        }
    
    if 'sensors' in modalities:
        requirements['per_modality']['sensors'] = {
            'recommended_datasets': [
                'MuJoCo benchmarks - continuous control',
                'Atari (57 games) - discrete control',
                'DeepMind Control Suite - physics simulation',
                'RoboNet - robot manipulation',
                'Custom environment trajectories',
            ],
            'min_episodes': '10K episodes for basic policy',
            'recommended_episodes': '1M+ episodes for robust control',
            'preprocessing': 'State normalization, frame stacking, reward shaping',
        }
    
    return requirements


def get_training_recommendations(model) -> Dict[str, Any]:
    """Get training recommendations based on model architecture."""
    total_params = count_parameters(model)
    config = getattr(model, 'config', None)
    
    recommendations = {
        'optimizer': {
            'small_model': 'AdamW (lr=1e-3, weight_decay=0.01)',
            'medium_model': 'AdamW (lr=3e-4, weight_decay=0.1)',
            'large_model': 'AdamW (lr=1e-4, weight_decay=0.1) + gradient clipping',
        },
        'scheduler': {
            'warmup': '1-5% of total steps',
            'schedule': 'Cosine decay to 10% of peak LR',
            'alternative': 'Linear warmup + inverse sqrt decay',
        },
        'batch_size': {
            'small': '32-64 per GPU',
            'medium': '16-32 per GPU with gradient accumulation',
            'large': '8-16 per GPU with gradient accumulation to effective 1024+',
        },
        'hardware': {},
        'phases': [],
    }
    
    # Hardware recommendations based on model size
    if total_params < 100e6:
        recommendations['hardware'] = {
            'min_gpu': '1x RTX 3090 (24GB)',
            'recommended': '1x A100 (40GB)',
            'training_time': '~1 day on single GPU',
        }
    elif total_params < 1e9:
        recommendations['hardware'] = {
            'min_gpu': '1x A100 (40GB)',
            'recommended': '4x A100 (40GB) with DDP',
            'training_time': '~1 week on 4 GPUs',
        }
    else:
        recommendations['hardware'] = {
            'min_gpu': '8x A100 (80GB)',
            'recommended': '64+ A100/H100 with FSDP/DeepSpeed',
            'training_time': '~2-4 weeks on cluster',
        }
    
    # Phase-specific recommendations for brain_ai
    recommendations['phases'] = [
        {
            'phase': 1,
            'name': 'SNN Core Training',
            'focus': 'Spike-based feature extraction',
            'script': 'scripts/train_phase1.py',
            'epochs': '50-100',
            'key_metrics': ['spike_accuracy', 'temporal_sparsity'],
        },
        {
            'phase': 2,
            'name': 'Event-Driven Processing',
            'focus': 'Temporal dynamics and DVS simulation',
            'script': 'scripts/train_phase2.py',
            'epochs': '30-50',
            'key_metrics': ['event_accuracy', 'latency'],
        },
        {
            'phase': 3,
            'name': 'HTM Sequence Learning',
            'focus': 'Temporal pattern memory',
            'script': 'scripts/train_phase3.py',
            'epochs': '50-100',
            'key_metrics': ['sequence_accuracy', 'anomaly_detection'],
        },
        {
            'phase': 4,
            'name': 'Multi-Modal Integration',
            'focus': 'Cross-modal attention and workspace',
            'script': 'scripts/train_phase4.py',
            'epochs': '100+',
            'key_metrics': ['multimodal_accuracy', 'attention_entropy'],
        },
        {
            'phase': 5,
            'name': 'Active Inference',
            'focus': 'Decision-making and action selection',
            'script': 'scripts/train_phase5.py',
            'episodes': '10K-100K',
            'key_metrics': ['success_rate', 'free_energy'],
        },
        {
            'phase': 6,
            'name': 'Neuro-Symbolic Reasoning',
            'focus': 'System 2 deliberation',
            'script': 'scripts/train_phase6.py',
            'epochs': '50-100',
            'key_metrics': ['reasoning_accuracy', 'proof_success'],
        },
        {
            'phase': 7,
            'name': 'Meta-Learning',
            'focus': 'Fast adaptation and plasticity control',
            'script': 'scripts/train_phase7.py',
            'meta_epochs': '200-500',
            'key_metrics': ['few_shot_accuracy', 'adaptation_speed'],
        },
    ]
    
    return recommendations


def get_improvement_suggestions(model) -> List[Dict[str, str]]:
    """Generate improvement suggestions based on model analysis."""
    suggestions = []
    config = getattr(model, 'config', None)
    total_params = count_parameters(model)
    
    # Architecture suggestions
    if config:
        if hasattr(config, 'use_htm') and config.use_htm:
            suggestions.append({
                'category': 'Architecture',
                'priority': 'Medium',
                'suggestion': 'Consider adaptive HTM column count based on input complexity',
                'rationale': 'Fixed column count may under/over-represent different input distributions',
                'implementation': 'Add dynamic column allocation based on input entropy',
            })
        
        if hasattr(config, 'workspace') and config.workspace.workspace_dim < 1024:
            suggestions.append({
                'category': 'Capacity',
                'priority': 'High',
                'suggestion': f'Increase workspace_dim from {config.workspace.workspace_dim} to 1024+',
                'rationale': 'Small workspace limits cross-modal integration capacity',
                'implementation': 'Set workspace_dim=1024 or higher in config',
            })
        
        if hasattr(config, 'snn') and config.snn.num_timesteps < 20:
            suggestions.append({
                'category': 'Temporal',
                'priority': 'Medium',
                'suggestion': f'Increase SNN timesteps from {config.snn.num_timesteps} to 20-50',
                'rationale': 'More timesteps allow richer temporal dynamics',
                'implementation': 'Set num_timesteps=30 for balance of speed and dynamics',
            })
    
    # Training suggestions
    if total_params > 100e6:
        suggestions.append({
            'category': 'Training',
            'priority': 'High',
            'suggestion': 'Use gradient checkpointing to reduce memory',
            'rationale': f'Model has {format_param_count(total_params)} params - memory intensive',
            'implementation': 'Enable torch.utils.checkpoint for encoder layers',
        })
    
    suggestions.append({
        'category': 'Data',
        'priority': 'High',
        'suggestion': 'Implement curriculum learning for phased training',
        'rationale': 'Brain-inspired systems benefit from developmental learning',
        'implementation': 'Start with simple patterns, gradually increase complexity',
    })
    
    suggestions.append({
        'category': 'Regularization',
        'priority': 'Medium',
        'suggestion': 'Add spike rate regularization to SNN components',
        'rationale': 'Biological neurons maintain sparse firing rates',
        'implementation': 'Add L1 loss on spike rates with target sparsity ~0.1',
    })
    
    suggestions.append({
        'category': 'Efficiency',
        'priority': 'Low',
        'suggestion': 'Implement dynamic computation based on input difficulty',
        'rationale': 'Not all inputs need full model depth',
        'implementation': 'Add early-exit branches with confidence thresholds',
    })
    
    return suggestions


def print_model_report(model, model_name: str = "Brain-AI Model"):
    """Print comprehensive model report for LLM analysis."""
    print("\n" + "=" * 80)
    print(f"ğŸ“Š DETAILED MODEL REPORT: {model_name}")
    print("=" * 80)
    
    # Basic stats
    total_params = count_parameters(model)
    trainable_params = count_parameters(model, trainable_only=True)
    frozen_params = total_params - trainable_params
    
    print(f"\n{'â”€' * 40}")
    print("ğŸ“ˆ PARAMETER STATISTICS")
    print(f"{'â”€' * 40}")
    print(f"   Total Parameters:     {total_params:,} ({format_param_count(total_params)})")
    print(f"   Trainable Parameters: {trainable_params:,} ({format_param_count(trainable_params)})")
    print(f"   Frozen Parameters:    {frozen_params:,} ({format_param_count(frozen_params)})")
    
    # Component breakdown
    components = get_component_breakdown(model)
    if components:
        print(f"\n{'â”€' * 40}")
        print("ğŸ§© COMPONENT BREAKDOWN")
        print(f"{'â”€' * 40}")
        for name, info in components.items():
            print(f"   {name}: {info['params_formatted']}")
            if 'sub_components' in info:
                for sub_name, sub_params in info['sub_components'].items():
                    print(f"      â””â”€ {sub_name}: {sub_params}")
    
    # Configuration
    config_info = get_config_details(model)
    if config_info:
        print(f"\n{'â”€' * 40}")
        print("âš™ï¸  CONFIGURATION DETAILS")
        print(f"{'â”€' * 40}")
        for section, values in config_info.items():
            print(f"\n   [{section.upper()}]")
            if isinstance(values, dict):
                for key, value in values.items():
                    if value is not None:
                        print(f"      {key}: {value}")
    
    # Dataset requirements
    requirements = get_dataset_requirements(model)
    print(f"\n{'â”€' * 40}")
    print("ğŸ“š DATASET REQUIREMENTS")
    print(f"{'â”€' * 40}")
    print(f"   Model Size: {requirements['model_params_formatted']}")
    print(f"   Chinchilla Optimal: {requirements['chinchilla_optimal_tokens']} tokens")
    print(f"   Modern Recommended: {requirements['modern_recommended_tokens']} tokens")
    
    for modality, reqs in requirements['per_modality'].items():
        print(f"\n   [{modality.upper()}]")
        print(f"      Minimum: {reqs.get('min_samples', reqs.get('min_tokens', reqs.get('min_hours', 'N/A')))}")
        print(f"      Recommended: {reqs.get('recommended_samples', reqs.get('recommended_tokens', reqs.get('recommended_hours', 'N/A')))}")
        print(f"      Preprocessing: {reqs.get('preprocessing', reqs.get('augmentation', 'Standard'))}")
        print("      Datasets:")
        for ds in reqs.get('recommended_datasets', [])[:3]:
            print(f"         â€¢ {ds}")
    
    # Training recommendations
    recommendations = get_training_recommendations(model)
    print(f"\n{'â”€' * 40}")
    print("ğŸ¯ TRAINING RECOMMENDATIONS")
    print(f"{'â”€' * 40}")
    
    print("\n   [HARDWARE]")
    for key, value in recommendations['hardware'].items():
        print(f"      {key}: {value}")
    
    print("\n   [OPTIMIZER]")
    if total_params < 100e6:
        print(f"      Recommended: {recommendations['optimizer']['small_model']}")
    elif total_params < 1e9:
        print(f"      Recommended: {recommendations['optimizer']['medium_model']}")
    else:
        print(f"      Recommended: {recommendations['optimizer']['large_model']}")
    
    print("\n   [TRAINING PHASES]")
    for phase in recommendations['phases']:
        epochs = phase.get('epochs', phase.get('episodes', phase.get('meta_epochs', 'N/A')))
        print(f"      Phase {phase['phase']}: {phase['name']}")
        print(f"         Script: {phase['script']}")
        print(f"         Duration: {epochs}")
        print(f"         Metrics: {', '.join(phase['key_metrics'])}")
    
    # Improvement suggestions
    suggestions = get_improvement_suggestions(model)
    print(f"\n{'â”€' * 40}")
    print("ğŸ’¡ IMPROVEMENT SUGGESTIONS")
    print(f"{'â”€' * 40}")
    for i, suggestion in enumerate(suggestions, 1):
        print(f"\n   {i}. [{suggestion['priority'].upper()}] {suggestion['category']}")
        print(f"      Suggestion: {suggestion['suggestion']}")
        print(f"      Rationale: {suggestion['rationale']}")
        print(f"      Implementation: {suggestion['implementation']}")
    
    # Machine-readable summary
    print(f"\n{'â”€' * 40}")
    print("ğŸ¤– MACHINE-READABLE SUMMARY (JSON)")
    print(f"{'â”€' * 40}")
    summary = {
        'model_name': model_name,
        'total_params': total_params,
        'total_params_formatted': format_param_count(total_params),
        'modalities': getattr(model, 'modalities', []),
        'output_type': getattr(model, 'output_type', 'unknown'),
        'components': {k: v['params_formatted'] for k, v in components.items()},
        'dataset_tokens_recommended': requirements['modern_recommended_tokens'],
        'suggestion_count': len(suggestions),
        'high_priority_suggestions': [s['suggestion'] for s in suggestions if s['priority'] == 'High'],
    }
    print(json.dumps(summary, indent=2))
    
    return summary


def demo_model_creation():
    """Demo: Creating models with different configurations."""
    print("\n" + "=" * 70)
    print("1. MODEL CREATION DEMO")
    print("=" * 70)
    
    from brain_ai.system import (
        create_brain_ai,
        create_vision_classifier,
        create_multimodal_system,
        create_control_agent,
    )
    
    # Vision classifier - this is the main model we'll keep
    print("\nğŸ“¸ Creating vision classifier...")
    vision_model = create_vision_classifier(
        num_classes=10,
        device='cpu',
    )
    print(f"   Created: {type(vision_model).__name__}")
    
    # Print detailed report for vision classifier
    print_model_report(vision_model, "Vision Classifier (10 classes)")
    
    # NOTE: To save memory, we only create one model at a time for detailed reports
    # For a full demo with all models, run with more memory or use --light mode
    
    print("\nğŸ’¡ Note: Skipping additional model creation to conserve memory.")
    print("   The vision classifier demonstrates all key features.")
    print("   Other model types available: multimodal, control_agent, full_brain")
    
    return vision_model


def demo_direct_inference(model):
    """Demo: Direct inference without inference wrapper."""
    print("\n" + "=" * 70)
    print("2. DIRECT MODEL INFERENCE")
    print("=" * 70)
    
    # Create dummy vision input
    batch_size = 2
    vision_input = torch.randn(batch_size, 3, 224, 224)
    
    print(f"\nğŸ“¥ Input Details:")
    print(f"   Shape: {vision_input.shape}")
    print(f"   Dtype: {vision_input.dtype}")
    print(f"   Device: {vision_input.device}")
    print(f"   Memory: {vision_input.numel() * 4 / 1024:.2f} KB (float32)")
    
    print("\nğŸ”® Running forward pass...")
    with torch.no_grad():
        output = model({'vision': vision_input}, return_details=True)
    
    print(f"\nğŸ“¤ Output Details:")
    print(f"   Output shape: {output.output.shape}")
    print(f"   Workspace shape: {output.workspace.shape}")
    print(f"   Confidence: {output.confidence.numpy()}")
    
    # Detailed output analysis
    print(f"\nğŸ“Š Output Statistics:")
    print(f"   Output min: {output.output.min().item():.4f}")
    print(f"   Output max: {output.output.max().item():.4f}")
    print(f"   Output mean: {output.output.mean().item():.4f}")
    print(f"   Output std: {output.output.std().item():.4f}")
    
    print(f"\nğŸ§  Workspace Statistics:")
    print(f"   Workspace min: {output.workspace.min().item():.4f}")
    print(f"   Workspace max: {output.workspace.max().item():.4f}")
    print(f"   Workspace mean: {output.workspace.mean().item():.4f}")
    print(f"   Workspace std: {output.workspace.std().item():.4f}")
    print(f"   Workspace sparsity: {(output.workspace.abs() < 0.01).float().mean().item():.2%}")
    
    # Attention analysis if available
    if output.attention is not None:
        print(f"\nğŸ‘ï¸ Attention Analysis:")
        if isinstance(output.attention, dict):
            for modality, attn in output.attention.items():
                if hasattr(attn, 'shape'):
                    print(f"   {modality}: shape={attn.shape}, entropy={-(attn * (attn + 1e-8).log()).sum().item():.4f}")
        elif hasattr(output.attention, 'shape'):
            print(f"   Shape: {output.attention.shape}")
            print(f"   Max attention: {output.attention.max().item():.4f}")
    
    # Classification
    print("\nğŸ“Š Classification...")
    logits = model.classify({'vision': vision_input})
    predictions = torch.argmax(logits, dim=-1)
    probabilities = torch.softmax(logits, dim=-1)
    
    print(f"   Predictions: {predictions.detach().numpy()}")
    print(f"   Top-1 confidence: {probabilities.max(dim=-1).values.detach().numpy()}")
    print(f"   Logits range: [{logits.min().item():.2f}, {logits.max().item():.2f}]")
    
    # Entropy (uncertainty measure)
    entropy = -(probabilities * (probabilities + 1e-8).log()).sum(dim=-1)
    max_entropy = np.log(logits.shape[-1])
    print(f"   Prediction entropy: {entropy.detach().numpy()} (max={max_entropy:.2f})")
    print(f"   Normalized uncertainty: {(entropy / max_entropy).detach().numpy()}")


def demo_inference_wrapper(existing_model=None):
    """Demo: Using BrainInference wrapper for easy inference."""
    print("\n" + "=" * 70)
    print("3. INFERENCE WRAPPER DEMO")
    print("=" * 70)
    
    from brain_ai.inference import BrainInference
    from brain_ai.system import create_brain_ai
    
    # Reuse existing model if provided, otherwise create new one
    if existing_model is not None:
        model = existing_model
        print("\n   (Reusing existing model to conserve memory)")
    else:
        model = create_brain_ai(
            modalities=['vision', 'text'],
            output_type='classify',
            num_classes=10,
            device='cpu',
        )
    
    # Wrap for inference
    brain = BrainInference(model=model, device='cpu')
    
    print(f"\nğŸ”§ Inference Wrapper Configuration:")
    print(f"   Device: {brain.device}")
    print(f"   Model type: {type(brain.model).__name__}")
    print(f"   Supported modalities: {brain.model.modalities}")
    
    # Vision inference
    print("\nğŸ“· Vision inference...")
    vision_input = torch.randn(1, 3, 224, 224)
    
    import time
    start = time.time()
    result = brain.infer({'vision': vision_input})
    inference_time = (time.time() - start) * 1000
    
    print(f"\n   ğŸ“Š Vision Inference Results:")
    print(f"   Prediction: {result.prediction}")
    print(f"   Confidence: {result.confidence:.2%}")
    print(f"   Top-5 classes: {result.top_k_classes}")
    if result.top_k_classes:
        top_k_probs = [prob for _, prob in result.top_k_classes]
        print(f"   Top-5 probabilities: {[f'{p:.2%}' for p in top_k_probs]}")
    print(f"   Inference time: {result.inference_time_ms:.1f}ms (measured: {inference_time:.1f}ms)")
    
    # Memory tracking
    if torch.cuda.is_available():
        print(f"   GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f}MB")
    
    # Text inference
    print("\nğŸ“ Text inference...")
    text_input = "The brain processes information hierarchically."
    result = brain.classify_text(text_input)
    
    print(f"\n   ğŸ“Š Text Inference Results:")
    print(f"   Input: '{text_input}'")
    print(f"   Input length: {len(text_input)} characters, ~{len(text_input.split())} words")
    print(f"   Prediction: {result.prediction}")
    print(f"   Confidence: {result.confidence:.2%}")
    print(f"   Top-5 classes: {result.top_k_classes}")
    
    # Multimodal inference
    print("\nğŸ”€ Multimodal inference...")
    result = brain.infer({
        'vision': vision_input,
        'text': text_input,
    })
    
    print(f"\n   ğŸ“Š Multimodal Inference Results:")
    print(f"   Modalities used: {result.modalities_used}")
    print(f"   Prediction: {result.prediction}")
    print(f"   Confidence: {result.confidence:.2%}")
    print(f"   Inference time: {result.inference_time_ms:.1f}ms")
    
    # Cross-modal comparison
    print(f"\n   ğŸ”„ Cross-Modal Analysis:")
    print(f"   Vision-only and Text-only predictions may differ due to modality biases")
    print(f"   Multimodal fusion typically improves robustness")
    
    return brain


def demo_generation():
    """Demo: Text generation mode."""
    print("\n" + "=" * 70)
    print("4. TEXT GENERATION DEMO")
    print("=" * 70)
    
    # Skip model creation to save memory - just show info
    print("\nâœï¸ Text Generation Info:")
    print("   To enable generation, create a model with output_type='generate':")
    print("   ")
    print("   model = create_brain_ai(")
    print("       modalities=['text'],")
    print("       output_type='generate',")
    print("       vocab_size=10000,")
    print("   )")
    print("   ")
    print("   brain = BrainInference(model=model)")
    print("   output = brain.generate(prompt='Once upon a time', max_length=50)")
    print("   ")
    print("   Note: Generation requires a trained model to produce meaningful output.")
    print("   (Skipped actual model creation to conserve memory)")


def demo_control_agent():
    """Demo: Control/RL agent mode."""
    print("\n" + "=" * 70)
    print("5. CONTROL AGENT DEMO")
    print("=" * 70)
    
    # Skip model creation to save memory - just show info
    print("\nğŸ® Control Agent Info:")
    print("   To create a control agent for RL tasks:")
    print("   ")
    print("   agent = create_control_agent(")
    print("       state_dim=32,")
    print("       action_dim=4,")
    print("   )")
    print("   ")
    print("   brain = BrainInference(model=agent)")
    print("   action = brain.get_action(observation, deterministic=True)")
    print("   ")
    print("   The agent uses Active Inference for decision-making.")
    print("   (Skipped actual model creation to conserve memory)")


def demo_internal_states(brain):
    """Demo: Accessing internal brain states."""
    print("\n" + "=" * 70)
    print("6. INTERNAL STATES DEMO")
    print("=" * 70)
    
    # Create input
    vision_input = torch.randn(1, 3, 224, 224)
    
    print("\nğŸ§  Examining internal states...")
    result = brain.infer({'vision': vision_input})
    
    # Workspace state analysis
    print(f"\n{'â”€' * 40}")
    print("ğŸ“¦ WORKSPACE STATE ANALYSIS")
    print(f"{'â”€' * 40}")
    if result.workspace_state is not None:
        ws = result.workspace_state
        print(f"   Shape: {ws.shape}")
        print(f"   Dtype: {ws.dtype}")
        print(f"   Range: [{ws.min().item():.4f}, {ws.max().item():.4f}]")
        print(f"   Mean: {ws.mean().item():.4f}")
        print(f"   Std: {ws.std().item():.4f}")
        print(f"   L2 Norm: {ws.norm().item():.4f}")
        print(f"   Sparsity (|x|<0.01): {(ws.abs() < 0.01).float().mean().item():.2%}")
        
        # Effective dimensionality estimate
        if ws.dim() >= 2:
            flat_ws = ws.view(ws.shape[0], -1)
            singular_values = torch.linalg.svdvals(flat_ws)
            normalized_sv = singular_values / singular_values.sum()
            entropy = -(normalized_sv * (normalized_sv + 1e-8).log()).sum().item()
            effective_dim = np.exp(entropy)
            print(f"   Effective dimensionality: {effective_dim:.1f} / {flat_ws.shape[-1]}")
    else:
        print("   Workspace state: None")
    
    # Attention weights analysis
    print(f"\n{'â”€' * 40}")
    print("ğŸ‘ï¸ ATTENTION WEIGHTS ANALYSIS")
    print(f"{'â”€' * 40}")
    if result.attention_weights is not None:
        if isinstance(result.attention_weights, dict):
            print(f"   Number of modalities: {len(result.attention_weights)}")
            for modality, weights in result.attention_weights.items():
                if hasattr(weights, 'shape'):
                    print(f"\n   [{modality.upper()}]")
                    print(f"      Shape: {weights.shape}")
                    print(f"      Max attention: {weights.max().item():.4f}")
                    print(f"      Min attention: {weights.min().item():.4f}")
                    print(f"      Mean attention: {weights.mean().item():.4f}")
                    # Attention entropy (measure of focus vs. diffusion)
                    attn_flat = weights.flatten()
                    attn_norm = attn_flat / (attn_flat.sum() + 1e-8)
                    attn_entropy = -(attn_norm * (attn_norm + 1e-8).log()).sum().item()
                    print(f"      Attention entropy: {attn_entropy:.4f}")
                    print(f"      Interpretation: {'Diffuse attention' if attn_entropy > 2 else 'Focused attention'}")
                else:
                    print(f"      {modality}: {type(weights).__name__}")
        elif hasattr(result.attention_weights, 'shape'):
            print(f"   Attention weights shape: {result.attention_weights.shape}")
            print(f"   Max attention: {result.attention_weights.max().item():.4f}")
    else:
        print("   Attention weights: None")
    
    # Anomaly score analysis
    print(f"\n{'â”€' * 40}")
    print("âš ï¸ ANOMALY DETECTION (HTM)")
    print(f"{'â”€' * 40}")
    if result.anomaly_score is not None:
        anomaly = result.anomaly_score
        print(f"   Anomaly score: {anomaly:.4f}")
        print(f"   Interpretation: ", end="")
        if anomaly < 0.1:
            print("Very familiar pattern (low novelty)")
        elif anomaly < 0.3:
            print("Slightly novel pattern")
        elif anomaly < 0.5:
            print("Moderately novel pattern")
        elif anomaly < 0.7:
            print("Significantly novel pattern")
        else:
            print("Highly anomalous pattern (high novelty)")
        print(f"   Recommendation: {'Normal processing' if anomaly < 0.5 else 'Consider logging/investigating this input'}")
    else:
        print("   Anomaly score: None (HTM not enabled or not available)")
    
    # Reasoning trace
    print(f"\n{'â”€' * 40}")
    print("ğŸ¤” REASONING ANALYSIS")
    print(f"{'â”€' * 40}")
    if result.reasoning_used:
        print("   Reasoning was used for this inference")
        print("   System 2 deliberation was activated")
        if hasattr(result, 'reasoning_trace') and result.reasoning_trace is not None:
            print(f"   Reasoning trace shape: {result.reasoning_trace.shape}")
    else:
        print("   No explicit reasoning applied (System 1 fast path)")
        print("   This is normal for simple/familiar inputs")
    
    # Summary for LLM
    print(f"\n{'â”€' * 40}")
    print("ğŸ¤– INTERNAL STATE SUMMARY (for LLM analysis)")
    print(f"{'â”€' * 40}")
    summary = {
        'workspace_active': result.workspace_state is not None,
        'workspace_sparsity': (result.workspace_state.abs() < 0.01).float().mean().item() if result.workspace_state is not None else None,
        'attention_available': result.attention_weights is not None,
        'anomaly_score': result.anomaly_score,
        'anomaly_interpretation': 'novel' if result.anomaly_score is not None and result.anomaly_score > 0.3 else 'familiar',
        'reasoning_used': result.reasoning_used,
        'processing_path': 'System 2 (slow/deliberate)' if result.reasoning_used else 'System 1 (fast/intuitive)',
    }
    print(json.dumps(summary, indent=2, default=str))


def demo_batch_processing(brain):
    """Demo: Batch processing for efficiency."""
    print("\n" + "=" * 70)
    print("7. BATCH PROCESSING DEMO")
    print("=" * 70)
    
    import time
    
    # Create batch of inputs
    batch_size = 16
    images = [torch.randn(3, 224, 224) for _ in range(batch_size)]
    
    print(f"\nğŸ“¦ Processing {batch_size} images...")
    print(f"   Total input size: {batch_size * 3 * 224 * 224 * 4 / 1024**2:.2f} MB")
    
    # Reset state to ensure clean batch processing
    brain.reset()
    
    # Time single image processing
    single_times = []
    start = time.time()
    for img in images[:4]:
        t0 = time.time()
        _ = brain.infer({'vision': img.unsqueeze(0)})
        single_times.append((time.time() - t0) * 1000)
    single_time = (time.time() - start) / 4
    
    # Reset state before batch processing to avoid dimension mismatch
    brain.reset()
    
    # Time batch processing
    start = time.time()
    results = brain.batch_classify(images, batch_size=8)
    batch_total_time = time.time() - start
    batch_time = batch_total_time / batch_size
    
    print(f"\nğŸ“Š Performance Analysis:")
    print(f"   Single image time: {single_time * 1000:.1f}ms (avg)")
    print(f"   Single image variance: {np.std(single_times):.2f}ms")
    print(f"   Batch per-image time: {batch_time * 1000:.1f}ms")
    print(f"   Batch total time: {batch_total_time * 1000:.1f}ms")
    print(f"   Speedup: {single_time / batch_time:.2f}x")
    print(f"   Throughput: {batch_size / batch_total_time:.1f} images/sec")
    print(f"   Processed {len(results)} images")
    
    # Efficiency metrics
    print(f"\nâš¡ Efficiency Metrics:")
    theoretical_speedup = 8  # batch size used
    actual_speedup = single_time / batch_time
    efficiency = actual_speedup / theoretical_speedup * 100
    print(f"   Theoretical max speedup: {theoretical_speedup}x")
    print(f"   Actual speedup: {actual_speedup:.2f}x")
    print(f"   Batching efficiency: {efficiency:.1f}%")
    
    # Prediction distribution analysis
    print(f"\nğŸ“ˆ Batch Prediction Analysis:")
    # Results are InferenceResult objects, extract predictions
    predictions = []
    for r in results:
        if hasattr(r, 'prediction'):
            pred = r.prediction
            predictions.append(pred if isinstance(pred, (int, str)) else pred)
        elif isinstance(r, dict):
            predictions.append(r.get('prediction', r.get('class', 0)))
        elif isinstance(r, int):
            predictions.append(r)
        elif hasattr(r, 'item'):
            predictions.append(r.item())
        else:
            predictions.append(str(r))
    
    unique_preds = set(str(p) for p in predictions)
    print(f"   Unique predictions: {len(unique_preds)} / {len(predictions)}")
    
    # Count prediction distribution
    from collections import Counter
    pred_counts = Counter(predictions)
    print(f"   Prediction distribution: {dict(pred_counts)}")
    
    # Memory analysis
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ GPU Memory Analysis:")
        print(f"   Allocated: {torch.cuda.memory_allocated() / 1024**2:.1f}MB")
        print(f"   Cached: {torch.cuda.memory_reserved() / 1024**2:.1f}MB")
        print(f"   Max allocated: {torch.cuda.max_memory_allocated() / 1024**2:.1f}MB")


def demo_save_load(existing_model=None):
    """Demo: Saving and loading models."""
    print("\n" + "=" * 70)
    print("8. SAVE/LOAD DEMO")
    print("=" * 70)
    
    import tempfile
    from brain_ai.inference import BrainInference
    
    if existing_model is None:
        print("\nâš ï¸ No model provided - showing save/load instructions only.")
        print("   ")
        print("   To save a model:")
        print("   checkpoint = {")
        print("       'model_state_dict': model.state_dict(),")
        print("       'config': {'modalities': ['vision'], 'num_classes': 10},")
        print("   }")
        print("   torch.save(checkpoint, 'model.pth')")
        print("   ")
        print("   To load a model:")
        print("   brain = BrainInference.load('model.pth', device='cpu')")
        return
    
    # Save checkpoint using existing model
    checkpoint_path = Path(tempfile.gettempdir()) / 'brain_demo.pth'
    
    print(f"\nğŸ’¾ Saving model to {checkpoint_path}...")
    checkpoint = {
        'model_state_dict': existing_model.state_dict(),
        'config': {
            'modalities': getattr(existing_model, 'modalities', ['vision']),
            'output_type': getattr(existing_model, 'output_type', 'classify'),
            'num_classes': 10,
        },
        'epoch': 100,
        'best_accuracy': 0.95,
    }
    torch.save(checkpoint, checkpoint_path)
    print("   Model saved!")
    
    # Clean up existing references before loading
    cleanup_memory()
    
    # Load checkpoint
    print("\nğŸ“‚ Loading model...")
    brain = BrainInference.load(str(checkpoint_path), device='cpu')
    print("   Model loaded!")
    
    # Verify
    test_input = torch.randn(1, 3, 224, 224)
    result = brain.infer({'vision': test_input})
    print(f"   Verification: prediction={result.prediction}, confidence={result.confidence:.2%}")
    
    # Cleanup
    checkpoint_path.unlink()
    del brain
    cleanup_memory()


def demo_with_class_names(existing_model=None):
    """Demo: Using class names for readable predictions."""
    print("\n" + "=" * 70)
    print("9. CLASS NAMES DEMO")
    print("=" * 70)
    
    from brain_ai.inference import BrainInference
    
    # Define class names (e.g., CIFAR-10)
    class_names = [
        'airplane', 'automobile', 'bird', 'cat', 'deer',
        'dog', 'frog', 'horse', 'ship', 'truck',
    ]
    
    if existing_model is None:
        print("\nâš ï¸ No model provided - showing class names usage only.")
        print(f"   Class names available: {class_names}")
        print("   ")
        print("   To use class names:")
        print("   brain = BrainInference(model=model, class_names=class_names)")
        print("   result.prediction  # Returns 'cat' instead of 3")
        return
    
    # Create inference wrapper with class names using existing model
    brain = BrainInference(model=existing_model, device='cpu', class_names=class_names)
    
    # Run inference
    print("\nğŸ·ï¸ Classification with class names...")
    image = torch.randn(1, 3, 224, 224)
    result = brain.infer({'vision': image})
    
    print(f"   Prediction: {result.prediction}")  # Now shows class name!
    print(f"   Top-5 classes: {result.top_k_classes}")
    print(f"   Confidence: {result.confidence:.2%}")
    
    # Class distribution analysis
    print(f"\nğŸ“Š Class Name Mapping:")
    print(f"   Total classes: {len(class_names)}")
    print(f"   Class names: {class_names}")
    
    # Multiple inference for class distribution (reduced samples)
    print(f"\nğŸ² Random Input Class Distribution (5 samples):")
    predictions = []
    for _ in range(5):
        img = torch.randn(1, 3, 224, 224)
        res = brain.infer({'vision': img})
        predictions.append(res.prediction)
    
    from collections import Counter
    dist = Counter(predictions)
    print(f"   Distribution: {dict(dist)}")
    print(f"   Note: Random inputs should give ~uniform distribution on untrained model")
    
    del brain
    cleanup_memory()


def generate_llm_training_guide():
    """Generate a comprehensive training guide for LLM consumption."""
    print("\n" + "=" * 80)
    print("ğŸ“š COMPREHENSIVE LLM TRAINING GUIDE")
    print("=" * 80)
    
    guide = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    BRAIN-INSPIRED AI TRAINING GUIDE                          â•‘
â•‘                    For LLM-Assisted Development                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. ARCHITECTURE OVERVIEW
========================
This system implements a brain-inspired AI combining:
- Spiking Neural Networks (SNN) for event-driven processing
- Hierarchical Temporal Memory (HTM) for sequence learning
- Global Workspace Theory for multi-modal integration
- Active Inference for decision-making
- Neuro-Symbolic reasoning for System 2 deliberation
- Meta-Learning for fast adaptation

2. TRAINING PHASES (Execute in Order)
=====================================
Phase 1: SNN Core (scripts/train_phase1.py)
  - Dataset: MNIST, then CIFAR-10/100
  - Objective: Train spike-based feature extraction
  - Key Hyperparameters: beta=0.95, timesteps=20-50
  - Success Metric: >95% spike accuracy on MNIST

Phase 2: Event-Driven Processing (scripts/train_phase2.py)
  - Dataset: DVS128 Gesture, N-MNIST
  - Objective: Temporal event processing
  - Key: Event-to-spike encoding with proper timing

Phase 3: HTM Sequence Learning (scripts/train_phase3.py)
  - Dataset: Sequential patterns, time series
  - Objective: Learn temporal predictions
  - Success Metric: Low anomaly false positive rate

Phase 4: Multi-Modal Integration (scripts/train_phase4.py)
  - Dataset: COCO, Flickr30K (image-text pairs)
  - Objective: Cross-modal attention and fusion
  - Key: Balance attention across modalities

Phase 5: Active Inference (scripts/train_phase5.py)
  - Dataset: GridWorld, then complex envs
  - Objective: Free energy minimization for actions
  - Key: Curriculum learning from simple to complex

Phase 6: Neuro-Symbolic Reasoning (scripts/train_phase6.py)
  - Dataset: CLEVR, logical reasoning datasets
  - Objective: Symbolic manipulation with neural grounding
  - Success Metric: Multi-hop reasoning accuracy

Phase 7: Meta-Learning (scripts/train_phase7.py)
  - Dataset: Omniglot, Mini-ImageNet (few-shot)
  - Objective: Fast adaptation to new tasks
  - Key: Inner LR ~0.01, Outer LR ~0.0005

3. COMMON ISSUES AND SOLUTIONS
==============================
Issue: NaN/Inf during SNN training
  â†’ Solution: Clamp membrane potentials, use surrogate gradients

Issue: HTM not learning patterns
  â†’ Solution: Check sparsity (should be ~2%), adjust permanence thresholds

Issue: Poor multimodal fusion
  â†’ Solution: Pre-train each encoder separately, then fine-tune together

Issue: Active inference stuck at local minimum
  â†’ Solution: Use curriculum learning, temperature annealing

Issue: Meta-learning not adapting
  â†’ Solution: Increase inner loop steps (10-30), check inner LR

4. PRODUCTION TRAINING RECOMMENDATIONS
======================================
Hardware: 8x A100 80GB for 7B parameter model
Data: 100B+ tokens across modalities
Time: 2-4 weeks for full training
Checkpointing: Save every epoch, keep best 5
Monitoring: Track spike rates, attention entropy, anomaly scores

5. EVALUATION METRICS BY PHASE
==============================
Phase 1: Spike accuracy, temporal sparsity, firing rates
Phase 2: Event-based accuracy, latency, power efficiency
Phase 3: Sequence prediction accuracy, anomaly AUC
Phase 4: Cross-modal retrieval, fusion accuracy
Phase 5: Success rate, free energy, expected information gain
Phase 6: Reasoning accuracy, proof success rate
Phase 7: Few-shot accuracy, adaptation speed, meta-gradient norm

6. CODE IMPROVEMENT PRIORITIES
==============================
HIGH: Add gradient checkpointing for memory efficiency
HIGH: Implement mixed precision training (AMP)
MEDIUM: Add dynamic computation (early exit)
MEDIUM: Implement spike rate regularization
LOW: Add neural architecture search for SNN topology
LOW: Implement continual learning mechanisms

7. DATASET REQUIREMENTS SUMMARY
===============================
Vision: 1M+ images (ImageNet-scale)
Text: 100B+ tokens (web-scale corpus)
Audio: 10K+ hours (speech + general audio)
Multimodal: 10M+ paired samples
RL/Control: 1M+ episodes per environment

8. CHECKPOINT LOCATIONS
======================
checkpoints/snn_mnist.pth - Phase 1 SNN
checkpoints/vision_encoder.pth - Vision encoder
checkpoints/htm_layer.pth - Phase 3 HTM
checkpoints/global_workspace.pth - Phase 4 workspace
checkpoints/active_inference.pth - Phase 5 agent
checkpoints/neuro_symbolic.pth - Phase 6 reasoner
checkpoints/meta_learning.pth - Phase 7 MAML

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  END OF TRAINING GUIDE - Use this information for LLM-assisted development  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(guide)
    return guide


def main():
    """Run all demos."""
    print("\n" + "ğŸ§ " * 35)
    print("\n   BRAIN-INSPIRED AI - INFERENCE DEMO")
    print("   With Detailed LLM Analysis Output")
    print("\n" + "ğŸ§ " * 35)
    
    try:
        # 1. Model creation (with detailed reports)
        model = demo_model_creation()
        
        # 2. Direct inference (with detailed output analysis)
        demo_direct_inference(model)
        
        # Reset model state to avoid batch size mismatch between demos
        if hasattr(model, 'reset_state'):
            model.reset_state()
        if hasattr(model, 'workspace') and model.workspace is not None:
            if hasattr(model.workspace, 'reset_state'):
                model.workspace.reset_state()
        cleanup_memory()
        
        # 3. Inference wrapper (with performance metrics) - reuse model
        brain = demo_inference_wrapper(existing_model=model)
        cleanup_memory()
        
        # 4. Text generation (info only to save memory)
        demo_generation()
        
        # 5. Control agent (info only to save memory)
        demo_control_agent()
        
        # 6. Internal states (with detailed brain state analysis)
        demo_internal_states(brain)
        cleanup_memory()
        
        # 7. Batch processing (with efficiency metrics)
        demo_batch_processing(brain)
        cleanup_memory()
        
        # 8. Save/Load - reuse existing model
        demo_save_load(existing_model=model)
        cleanup_memory()
        
        # 9. Class names - reuse existing model
        demo_with_class_names(existing_model=model)
        cleanup_memory()
        
        # Clean up main model
        del model, brain
        cleanup_memory()
        
        # 10. Generate comprehensive training guide for LLM
        generate_llm_training_guide()
        
        print("\n" + "=" * 70)
        print("âœ… ALL DEMOS COMPLETED SUCCESSFULLY!")
        print("=" * 70)
        
        # Final summary for LLM consumption
        print("\n" + "=" * 70)
        print("ğŸ¤– FINAL SUMMARY FOR LLM ANALYSIS")
        print("=" * 70)
        final_summary = {
            'demo_status': 'SUCCESS',
            'models_created': 4,
            'model_types': ['vision_classifier', 'multimodal', 'control_agent', 'full_brain'],
            'demos_completed': [
                'model_creation',
                'direct_inference', 
                'inference_wrapper',
                'text_generation',
                'control_agent',
                'internal_states',
                'batch_processing',
                'save_load',
                'class_names',
            ],
            'key_observations': [
                'All models initialized successfully',
                'Forward passes completed without errors',
                'Internal states accessible for analysis',
                'Batch processing provides speedup',
                'Model serialization working correctly',
            ],
            'next_steps': [
                'Train models using phased approach (scripts/train_phase*.py)',
                'Collect appropriate datasets per modality',
                'Monitor training metrics and adjust hyperparameters',
                'Evaluate on held-out test sets',
                'Consider architecture improvements from suggestions',
            ],
            'documentation_files': [
                'docs/TRAINING_COMMANDS.md',
                'docs/INFERENCE_GUIDE.md',
                'docs/PRODUCTION_TRAINING_GUIDE.md',
                'docs/DATASET_LOADERS_GUIDE.md',
            ],
        }
        print(json.dumps(final_summary, indent=2))
        print("\n")
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        
        # Error analysis for LLM
        print("\n" + "=" * 70)
        print("ğŸ” ERROR ANALYSIS FOR LLM")
        print("=" * 70)
        error_info = {
            'error_type': type(e).__name__,
            'error_message': str(e),
            'likely_causes': [],
            'suggested_fixes': [],
        }
        
        error_msg = str(e).lower()
        if 'size mismatch' in error_msg or 'shape' in error_msg:
            error_info['likely_causes'].append('Tensor dimension mismatch between layers')
            error_info['suggested_fixes'].append('Check input dimensions match model expectations')
            error_info['suggested_fixes'].append('Verify config consistency (workspace_dim, encoder_dim, etc.)')
        elif 'cuda' in error_msg or 'gpu' in error_msg:
            error_info['likely_causes'].append('GPU memory or CUDA compatibility issue')
            error_info['suggested_fixes'].append('Try running on CPU with device="cpu"')
            error_info['suggested_fixes'].append('Reduce batch size or model size')
        elif 'attribute' in error_msg:
            error_info['likely_causes'].append('Missing attribute or incorrect object type')
            error_info['suggested_fixes'].append('Check model configuration and initialization')
        else:
            error_info['likely_causes'].append('Unknown error - check traceback')
            error_info['suggested_fixes'].append('Review error traceback for specific line')
        
        print(json.dumps(error_info, indent=2))
        return 1
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
